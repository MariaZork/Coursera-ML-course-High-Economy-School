# -*- coding: utf-8 -*-
"""
Created on Tue Apr  3 21:44:27 2018

@author: Maria
"""
#====Задание по программированию: Логистическая регрессия====
#1. Загрузите данные из файла data-logistic.csv. Это двумерная выборка, 
#целевая переменная на которой принимает значения -1 или 1.
#
#2. Убедитесь, что выше выписаны правильные формулы для градиентного спуска. 
#Обратите внимание, что мы используем полноценный градиентный спуск, а не его 
#стохастический вариант!
#
#3. Реализуйте градиентный спуск для обычной и L2-регуляризованной 
#(с коэффициентом регуляризации 10) логистической регрессии. Используйте 
#длину шага k=0.1. В качестве начального приближения используйте вектор (0, 0).
#
#4. Запустите градиентный спуск и доведите до сходимости 
#(евклидово расстояние между векторами весов на соседних итерациях должно 
#быть не больше 1e-5). Рекомендуется ограничить сверху 
#число итераций десятью тысячами.
#
#5. Какое значение принимает AUC-ROC на обучении без регуляризации и 
#при ее использовании? Эти величины будут ответом на задание. В качестве ответа 
#приведите два числа через пробел. Обратите внимание, что на вход функции 
#roc_auc_score нужно подавать оценки вероятностей, подсчитанные обученным 
#алгоритмом. Для этого воспользуйтесь сигмоидной функцией: 
#a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)).
#
#6. Попробуйте поменять длину шага. Будет ли сходиться алгоритм, если делать 
#более длинные шаги? Как меняется число итераций при уменьшении длины шага?
#
#7. Попробуйте менять начальное приближение. Влияет ли оно на что-нибудь?

import pandas as pd
import numpy as np

from sklearn.metrics import roc_auc_score

data = pd.read_csv('data-logistic.csv', sep=',', header=None)
X1 = data.iloc[:, 1]
X2 = data.iloc[:, 2]
Y = data.iloc[:, 0]

def grad_descent(X1, X2, Y, w1=0, w2=0, C=0, k=0.1, iters = 10000, error=1e-5):
    eps = 1    
    for i in range(iters):
        if eps > error:
            w1_save = w1
            w2_save = w2
            summa_w1 = np.sum(Y*X1*(1 - 1/(1 + np.exp(-Y*(w1*X1 + w2*X2)))))
            summa_w2 = np.sum(Y*X2*(1 - 1/(1 + np.exp(-Y*(w1*X1 + w2*X2)))))
            w1 += k*(1/len(Y))*summa_w1 - k*C*w1
            w2 += k*(1/len(Y))*summa_w2 - k*C*w2
            
            eps = np.sqrt((w1 - w1_save)**2 + (w2 - w2_save)**2)
        else:
            break
    return w1, w2

w1, w2 = grad_descent(X1, X2, Y)
probs = 1 / (1 + np.exp(-w1*X1 - w2*X2))
score = roc_auc_score(Y, probs)
print("AUC score when C=0:", score)

w1, w2 = grad_descent(X1, X2, Y, C=10)
probs = 1 / (1 + np.exp(-w1*X1 - w2*X2))
score = roc_auc_score(Y, probs)
print("AUC score when C=10:", score)
